---
title: "March-Machine_Learning_Madness"
author: "Lin Laiyi and Wong Jing Rui Henry (G2 Group 10)"
date: "November 2017"
output:
    html_document:
        theme: yeti
        highlight: tango
---

## Introduction
The National Collegiate Athletic Association (NCAA) organizes and manages colleges and university-level sports competions in the United States and Canada. In particular, the Divison I Men's Basketball Tournament is widly popular and brings in most of the revenue for NCAA. In fact, the 2017 NCAA Tournament is the most-watched in 24 years with viewers numbers averaging more than 9 million and an an all-time record 69.1 million live streams throughout the tournament.

The tournament takes place during March and April (hence, it is also known as "March Madness") after the regular season to determine the national championship.  68 teams are chosen, including the champions from 32 Division I conferences in the regular season, and 36 other teams chosen by an NCAA selection committee. The teams are organized into 4 regions, with each region comprising of 17 teams and the teams are ranked (or seeded) within their regions. 8 low-ranking teams plays a pre-tournament opening round ("First Four") for 4 places in the official tournament. The official March Madness tournament consist of 64 teams competing in 6 rounds of single-elimination "bracket". In these brackets, the teams are organized such that their next possible opponents are only from a pre-determined subset. The bracket is establised in such a way that teams play within their own region for 4 rounds, after which the regional champions are determined. The regional champions then play against each other for the national championnship in the next 2 rounds.

**In this project, we will build a classification model to predict the results of the 2017 NCAA March Madness tournament based on the teams' performance in the regular season.** Our task is to find a good model that fits the data from 2003-2016 and helps us predict out-of-sample (2017 NCAA March Madness tournament).

Data used in this project is from Kaggle (https://www.kaggle.com/c/march-machine-learning-mania-2017/data) which included team statistics from 2003-2017 in the regular season and results from the March Madness tournament from 2003-2016. Team statistics include typical basketball statistics such as field goals made, field goal attempted, defensive rebounds, steals, personal fouls etc.  

## The Data 
The data has been cleaned and prepared in an RData format. The data can be loaded from the NCAA.rdata file in the working directory. It consist of a train and test dataset for model building.An approximate 75:25 train-test split was performed on our full data set.The train dataset consist of data from 2003-2013 (713 data ponts). The test dataset consist of data from 2014-2016 (201 data points). In the RData file are also data from acutal team match-ups from round 1 to round 6 for the 2017 NCAA tournament which will be to check our model later. A tourney.run dataframe contains a fictitious tournament run generated by our model.

The datasets are pre-processed from the raw data from Kaggle. There are 2 types of data from Kaggle. The tournament data and the regular season data. For the tournament data, each record contain the results of match-ups between 2 opposing teams (team 1 and team 2) in the NCAA March Madness tournament from 2003-2016  **We will be predicting the probability of a victory for team 1 in our model**. A cutoff of 0.5 will be used to decide whether team 1 will win the match.  

For the regular season data, we first normalized the team statistics in all games of the regular season to a 40 minute game period based on the number of overtime. Next, we averaged the team statistics *within a given year* in the regular season for all the teams. We then combined this average statistics iwth the tournament matchups data. For every match-ups in the March Madness tournament from 2003-2016, we calculated the difference in the  average team statistics between team 1 and team 2 for the regular season in that correponding year. 

We also did some feature-engineering and created several efficiency measures commonly used in basketball such as the field goals efficiency and the assist-to-turnover ratio. Field goals efficiency and other similar efficiency measures are just the proportion of successful attempts(e.g. field goals made) over the total number of attempts(e.g. field goals attempted). Besides the teams' regular season statistics, other relevant statistics such as team's seed and team's region are also considered.

We first load and check the train dataset.
```{r}
load("NCAA.rdata")
summary(train)
```
Here, every row is a match up between 2 teams in the NCAA March Madness tournament. Below is a brief description of the columns.

- "Season": Year of the NCAA March Madness tournament
- "team1": Team 1 team ID
- "Daynum": Round number in the NCAA March Madness tournament
- "team2": Team 2 team ID
- "team1Victory": Dummy variable of whether team 1 won that particular match-up. *This will be used as our dependent variable in our classification model.*
- "team1Loc": Location of the match
- "score.diff": Difference in the average score in the regular season 
- "fgm.diff": Difference in the average field goals made in the regular season
- "fga.diff": Difference in the average field goals attempted in the regular season
- "fgm3.diff": Difference in the average three pointers made in the regular season
- "fga3.diff": Difference in the average three pointers attempted in the regular season
- "ftm.diff": Difference in the average free throws made in the regular season
- "fta.diff": Difference in the average free throws attempted in the regular season
- "or.diff": Difference in the average number of offensive rebounds in the regular season
- "dr.diff": Difference in the average number of defensive rebounds in the regular season
- "ast.diff": Difference in the average number of assists in the regular season
- "to.diff": Difference in the average number of turnovers in the regular season
- "stl.diff": Difference in the average number of steals in the regular season
- "blk.diff": Difference in the average number of blocks in the regular season
- "pf.diff": Difference in the average number of personal fouls in the regular season
- "fgmpct.diff": Difference in the average efficiency of field goals in the regular season
- "fgm3pct.diff": Difference in the average efficiency of three pointers in the regular season
- "ftmpct.diff": Difference in the average efficiency of free throws in the regular season
- "astto.diff": Difference in the average assist/turnover ratios in the regular season
- "Victory.diff": Difference in the average number of victories in the regular season
- "PureSeed.diff": Difference in seed given for the tournament(regardless of region)
- "region.diff": Dummy variable indicating whether team 1 and team 2 are in the same region.


The test dataset is organized in the same format. We select only useful variables for our model from the datasets, discarding variables like Season, team ID and match locations
```{r}
summary(test)

train<-train[,c(5,3,7:27)]
test<-test[,c(5,3,7:27)]
```

## The Models

### Model 1: Logistic Regression
We first start with a simple linear model. We fit a logistic regression model and use stepAIC for variable selection. The final model selected by stepAIC is team1Victory ~ Daynum + fgm.diff + fga.diff + fgm3.diff + fga3.diff + stl.diff + pf.diff + fgmpct.diff + fgm3pct.diff + PureSeed.diff. We also calculated the cv error and test error to be used for comparison with other models later.
```{r}
formula<-paste("team1Victory~",paste(colnames(train[,-1]),collapse="+"))
glm1<-glm(formula,train,family = binomial)
library("MASS")
step.both<-stepAIC(glm1,direction='both')
logis.best<-glm(step.both$formula,train,family=binomial)

library(boot)
cost <- function(y, p = 0) {mean(y != (p > 0.5))}
set.seed(123)
logis.cv.err<-cv.glm(train,logis.best,cost,10)$delta[1]
logis.pred<- predict(logis.best,test,type = "response")
logis.err<-mean(ifelse((as.numeric(logis.pred>=0.5) == test$team1Victory),0,1))
```

### Model 2: Elastic Net
We moved on to regularized linear models and fit an elastic net model to our training set. The alpha and lamda parameters are selected for a model with more coefficient shrinkage that is within 1 standard deviation from the minimum cv error. All variables shrink to zero except for PureSeed.diff. The difference in seed between team 1 and team 2 is highly predictive of the result of the match in the tournament. The negative coefficient indicates that a team will a lower seed will have a higher chance of winning. We also calculated the cv error and test error to be used for comparison with other models later.
```{r}
library(glmnet)
x<-model.matrix(team1Victory~.,train)[,-1]
y<-train[,1]
set.seed(123)
fold <- sample(rep(seq(10), length=nrow(x)))
alphas <- seq(0, 1, 0.1)
en.cv.error <- data.frame(alpha=alphas)
for (i in 1:length(alphas)){
  en.cv <- cv.glmnet(x, y, alpha=alphas[i], foldid=fold,family="binomial",type.measure = "class")
  en.cv.error[i, "lambda.1se"] <- en.cv$lambda.1se
  en.cv.error[i, "error.1se"] <- min(en.cv$cvm) + en.cv$cvsd[which.min(en.cv$cvm)]
}
en.lam2 <- en.cv.error[which.min(en.cv.error$error.1se), "lambda.1se"]
en.alpha <- en.cv.error[which.min(en.cv.error$error.1se), "alpha"]
en.mod<-glmnet(x,y,alpha=en.alpha,family="binomial")

en.cv.err<-min(en.cv.error$error.1se)
x.test<-model.matrix(team1Victory~.,test)[,-1]
en.pred<-predict(en.mod,newx = x.test, type="response", s=en.lam2, exact=T)
predict(en.mod,x,type="coefficient",s=en.lam2,exact=T)
en.err<-mean(ifelse((as.numeric(en.pred>=0.5) == test$team1Victory),0,1))
```

### Model 3: Random Forest
Next, we try fitting non-linear models from the decision tree family. A random forest model with 500 trees was built and mtry was tuned. We found that having 16 random variables at each split will minimized OOB error. From the variable importance plot, we again observed that the difference in seed between the teams will significantly affect the outcome of the match. The partial plot shows that a lower seed will have a higher chance of winning. We also calculated the cv error and test error to be used for comparison with other models later. 
```{r}
train$team1Victory<-as.factor(train$team1Victory)
library("randomForest")
set.seed(123) 
range<-ncol(train)-1
err.rfs <- rep(0, range)
for(m in 1:range){
  set.seed(123)
  rf <- randomForest(team1Victory ~ ., data=train, mtry=m)
  err.rfs[m] <- rf$err.rate[500]
}
plot(1:range, err.rfs, type="b", xlab="mtry", ylab="OOB Error")
rf.best<-randomForest(team1Victory ~ ., data=train, mtry=which.min(err.rfs))
varImpPlot(rf.best)
partialPlot(rf.best,train,x.var="PureSeed.diff",which.class=1)

rf.cv.err<-min(err.rfs)
test$team1Victory<-as.factor(test$team1Victory)
rf.pred<-predict(rf.best,test,type="vote")
rf.err<-mean(ifelse((as.numeric(rf.pred[,2]>=0.5) == test$team1Victory),0,1))

```

### Model 4: Boosted Trees
After testing for parallel grown trees, we next try sequentially grown boosted tree models using XgBoost. We tuned for max tree depth, eta(learning rate), subsample(data point sampling) and colsample (variable sampling) to minimize the cv error. Our best model has max tree depth = 3, eta = 0.05, subsample = 0.75 and colsample = 1. Again, our variable importance plot show that the differnce in seed between the teams is the most important vatiable in predicting the outcome of the match. Partial plot again shows that low seeds have higher chance of winning. We also calculated the cv error and test error to be used for comparison with other models later.
```{r}
library("xgboost")
x.train <- model.matrix(team1Victory ~ ., data=train)
y.train <- as.numeric(train[,1])-1
dtrain <- xgb.DMatrix(data=x.train, label=y.train)
objective <- "binary:logistic"
cv.fold <- 10
max_depths <- c(1,3,5)  
etas <- c(0.5,0.1,0.05)  
subsamples <- c(0.5,0.75,01)
colsamples <- c(0.6,0.8,1)
tune.out <- data.frame()
for (max_depth in max_depths) {
  for (eta in etas) {
    for (subsample in subsamples) {
      for (colsample in colsamples) {
        n.max <- round(100 / (eta * sqrt(max_depth)))
        set.seed(123)
        xgb.cv.fit <- xgb.cv(data = dtrain, objective=objective, nfold=cv.fold, early_stopping_rounds=150, verbose=0,
                             nrounds=n.max, max_depth=max_depth, eta=eta, subsample=subsample, colsample_bytree=colsample)
        n.best <- xgb.cv.fit$best_ntreelimit
        cv.err <- xgb.cv.fit$evaluation_log$test_error_mean[n.best]
        out <- data.frame(max_depth=max_depth, eta=eta, subsample=subsample, colsample=colsample, n.max=n.max, nrounds=n.best, cv.err=cv.err)
        tune.out <- rbind(tune.out, out)
      }      
    }
  }
}
opt <- which.min(tune.out$cv.err)
max_depth.opt <- tune.out$max_depth[opt]
eta.opt <- tune.out$eta[opt]
subsample.opt <- tune.out$subsample[opt]
colsample.opt <- tune.out$colsample[opt]
nrounds.opt <- tune.out$nrounds[opt]
set.seed(123)
tune.out[opt,]
xgb.best <- xgboost(data=dtrain, objective="binary:logistic", nround=nrounds.opt, max.depth=max_depth.opt, eta=eta.opt, subsample=subsample.opt, colsample_bytree=colsample.opt)
importance_matrix <- xgb.importance(model = xgb.best, feature_names = colnames(x.train))
xgb.plot.importance(importance_matrix=importance_matrix)
library("pdp")
plotPartial(partial(xgb.best, train=x.train, pred.var = "PureSeed.diff", chull = TRUE))

xgb.cv.err<-min(tune.out$cv.err)
x.test <- model.matrix(team1Victory ~ ., data=test)
xgb.pred <- predict(xgb.best, x.test)
xgb.err<-mean(ifelse((as.numeric(xgb.pred>=0.5) == test$team1Victory),0,1))
```

### Model 5: Support Vector Machines (Linear Kernel)
We next try another family of classifiers known as support vector machine. We first fit a svm model to the data using linear kernel and tuned the cost parameter. The best cost parameter is found to be 100. We also calculated the cv error and test error to be used for comparison with other models later.
```{r}
library(kernlab)
train$team1Victory<-as.factor(train$team1Victory)
set.seed(123)
err.svm <- rep(0, 5)
clist<-c(0.01,0.1,1,10,100)
for(c in 1:5){
  set.seed(123)
  svmmodel<-ksvm(team1Victory~.,train,kernel = "vanilladot",C=clist[c],prob.model=T,cross=10)
  err.svm[c] <- svmmodel@cross
}
svm.best<-ksvm(team1Victory~.,train,kernel = "vanilladot", C=clist[which.min(err.svm)], prob.model=T)

svm.cv.err<-min(err.svm)
svm.pred<-predict(svm.best,test,type="probabilities")
svm.err<-mean(ifelse((as.numeric(svm.pred[,2]>=0.5) == test$team1Victory),0,1))
```


### Model 6: Support Vector Machines (RBF Kernel)
Finally, we moved on to a non-linear fit using RBF kernel. We minimized cv error by tuning the cost parameter and sigma parameter of the RBF kernel. The best cost parameter was found to be 1 and the best sigma parameter is 0.01.  We also calculated the cv error and test error to be used for comparison with other models later.
```{r}
library(kernlab)
train$team1Victory<-as.factor(train$team1Victory)
set.seed(123) 
err.svm.rbf <- data.frame()
clist<-c(0.01,0.1,1,10,100)
sigmalist<-c(0.001,0.01,0.1,1)
for(c in clist){
  for (s in sigmalist){
    set.seed(123)
    svmmodel<-ksvm(team1Victory~.,train,kernel = "rbfdot",C=c,kpar=list(sigma=s),prob.model=T,cross=10)
    out <- data.frame(C=c, sigma=s, cv.err=svmmodel@cross)
    err.svm.rbf <- rbind(err.svm.rbf, out)
  }
}
opt <- which.min(err.svm.rbf$cv.err)
c.opt <- err.svm.rbf$C[opt]
sigma.opt <- err.svm.rbf$sigma[opt]
svm.rbf.best<-ksvm(team1Victory~.,train,kernel = "rbfdot", C=c.opt, kpar=list(sigma=sigma.opt), prob.model=T)
err.svm.rbf[opt,]

svm.rbf.cv.err<-min(err.svm.rbf$cv.err)
svm.rbf.pred<-predict(svm.rbf.best,test,type="probabilities")
svm.rbf.err<-mean(ifelse((as.numeric(svm.rbf.pred[,2]>=0.5)) == test$team1Victory,0,1))

```

### Model 7: Ensemble model (Weighted majority voting)
After building a good mix of linear and non-linear models from different families of classfiers, we try to combine these models together to give an overall prediction. we first try an ensemble model based on weighted majority voting. Prediction of each individual model is obtained from the test data and a majority voting scheme is used to decide the final prediction for the ensemble model. However, instead of all models having the one vote, the individual models are given weighted votes based on their train cv error. A model with lower cv error will be given a higher weights. Using this weighted majority voting method, a test error is calculated to be used for comparison with other models later.
```{r}
norm.cv.err<-(1/logis.cv.err)+(1/en.cv.err)+(1/rf.cv.err)+(1/xgb.cv.err)+(1/svm.cv.err)+(1/svm.rbf.cv.err)
logis.wt<-(1/logis.cv.err)/norm.cv.err
en.wt<-(1/en.cv.err)/norm.cv.err
rf.wt<-(1/rf.cv.err)/norm.cv.err
xgb.wt<-(1/xgb.cv.err)/norm.cv.err
svm.wt<-(1/svm.cv.err)/norm.cv.err
svm.rbf.wt<-(1/svm.rbf.cv.err)/norm.cv.err

ensemble.pred<- (logis.wt*(as.numeric(logis.pred>=0.5))+en.wt*(as.numeric(en.pred>=0.5))+rf.wt*(as.numeric(rf.pred[,2]>=0.5))+xgb.wt*(as.numeric(xgb.pred>=0.5))+svm.wt*(as.numeric(svm.pred[,2]>=0.5))+svm.rbf.wt*(as.numeric(svm.rbf.pred[,2]>=0.5)))
ensemble.err<-mean(ifelse((as.numeric(ensemble.pred>=0.5) == test$team1Victory),0,1))
```

### Model 7: Ensemble model (Logistic Regression)
Alternatively, the predicted probabilities for all the individual models was used as predictors and a logistic regression was fitted with team1Victory variable as the dependent variable. In this way, the individual models are weighted based on the coefficients of the logistic regression. Manual cross-validation is done for each model to obtained the predicted probabilities. Test error is also calculated for this ensemble model to be used for comparison with other models later.
```{r}
set.seed(123)
fold <- sample(rep(seq(10), length=nrow(train)))
##logistic regression
logis.prob<-rep(0,nrow(train))
for (k in 1:10) {
  logis.best<-glm(step.both$formula,train[fold != k,],family=binomial)
  logis.prob[fold==k] <- predict(logis.best, train[fold == k, ], type="response")
}
logis.train.pred<- logis.prob
##elastic net
en.prob<-rep(0,nrow(train))
for (k in 1:10) {
  x<-model.matrix(team1Victory~.,train[fold!=k,])[,-1]
  y<-train[fold!=k,1]
  en.mod<-glmnet(x,y,alpha=en.alpha,family="binomial")
  x.valid<-model.matrix(team1Victory~.,train[fold==k,])[,-1]
  en.prob[fold==k] <- predict(en.mod, newx=x.valid, type="response",s=en.lam2)
}
en.train.pred<-en.prob
##random forest
rf.prob<-rep(0,nrow(train))
for (k in 1:10) {
  rf.best<-randomForest(team1Victory ~ ., data=train[fold!=k,], mtry=which.min(err.rfs),xtest=train[fold==k,-1])
  rf.prob[fold==k]<-rf.best$test$votes[,2]
}
rf.train.pred<-rf.prob
##xgBoost
xgb.prob<-rep(0,nrow(train))
for (k in 1:10) {
  x.train <- model.matrix(team1Victory ~ ., data=train[fold!=k,])
  y.train <- as.numeric(train[fold!=k,1])-1
  dtrain <- xgb.DMatrix(data=x.train, label=y.train)
  xgb.best <- xgboost(data=dtrain, objective="binary:logistic", nround=nrounds.opt, max.depth=max_depth.opt, eta=eta.opt, subsample=subsample.opt, colsample_bytree=colsample.opt)
  x.test <- model.matrix(team1Victory ~ ., data=train[fold==k,])
  xgb.prob[fold==k] <- predict(xgb.best, x.test)
}
xgb.train.pred<-xgb.prob
##svm linear
svm.prob<-rep(0,nrow(train))
for (k in 1:10) {
  svm.best<-ksvm(team1Victory~.,train[fold!=k,],kernel = "vanilladot",C=clist[which.min(err.svm)],prob.model=T)
  svm.prob[fold==k]<-predict(svm.best,train[fold==k,],type="probabilities")[,2]
}
svm.train.pred<-svm.prob
##svm rbf
svm.rbf.prob<-rep(0,nrow(train))
for (k in 1:10) {
  svm.rbf.best<-ksvm(team1Victory~.,train[fold!=k,],kernel = "rbfdot",C=c.opt,kpar=list(sigma=sigma.opt),prob.model=T)
  svm.rbf.prob[fold==k]<-predict(svm.rbf.best,train[fold==k,],type="probabilities")[,2]
}
svm.rbf.train.pred<-svm.rbf.prob
##ensemble
ensemble.logis.cv<-data.frame(logis=logis.train.pred, elasticnet=en.train.pred, rf=rf.train.pred, xgb=xgb.train.pred, svm=svm.train.pred, svmrbf=svm.rbf.train.pred, y=train$team1Victory)
ensemble.logis.mod<-glm(y~.,ensemble.logis.cv,family=binomial)

ensemble.logis.test<-data.frame(logis=logis.pred, elasticnet=as.numeric(en.pred), rf=rf.pred[,2], xgb=xgb.pred, svm=svm.pred[,2], svmrbf=svm.rbf.pred[,2], y=test$team1Victory)
ensemble.logis.pred<-predict(ensemble.logis.mod,ensemble.logis.test,family=binomial,type="response")
ensemble.logis.err<-mean(ifelse((as.numeric(ensemble.logis.pred>=0.5) == test$team1Victory),0,1))

```

## Model Evaluation
The best individual model in terms of test error is the logistic regression model. Boosted trees performed well in the train data but is slightly overfitted resulting in a much higher test error. The ensemble models are effective in reducing the test error and **the ensemble model using logistic regression is the overall best model with a test error of 27.9%**. 


```{r}
table<-data.frame(model=c("Logistic Regression","Elastic Net","Random Forest","Boosted Trees","SVM Linear","SVM RBF","Ensemble(Weighted majority voting)","Ensemble(Logistic Regression)"), cv_error = c(logis.cv.err, min(en.cv.error$error.1se), min(err.rfs), min(tune.out$cv.err), min(err.svm), min(err.svm.rbf$cv.err), NA, NA),                test_error = c(logis.err, en.err, rf.err, xgb.err, svm.err, svm.rbf.err, ensemble.err, ensemble.logis.err))

table 
```

## 2017 NCAA Tornament prediction

### Actual Matches
To see how our model performs on the actual matches in the 2017 NCAA March Madness tournament, we used our model to predict the outcome of the matches played in the tournnament.The Kaggle competition where we got our data was before the tournament was played but the actual results from the tournament are now available (http://www.ncaa.com/interactive-bracket/basketball-men/d1). Hence, we can now compare our model predicitons to the actual results.

We first set up a function to enable us to get predicted probabilities for team 1 winning each match.
```{r}
model.test<-function(test){
  logis.pred<- predict(logis.best,test,type = "response")
  x.test<-data.matrix(test)
  en.pred<-predict(en.mod,newx = x.test, type="response", s=en.lam2)
  rf.best<-randomForest(team1Victory ~ ., data=train, mtry=which.min(err.rfs))
  rf.pred<-predict(rf.best,test,type="vote")
  x.test <- data.matrix(test)
  xgb.pred <- predict(xgb.best, x.test)
  svm.pred<-predict(svm.best,test,type="probabilities")
  svm.rbf.pred<-predict(svm.rbf.best,test,type="probabilities")
  ensemble.logis.test<-data.frame(logis=logis.pred,elasticnet=as.numeric(en.pred),rf=rf.pred[,2],xgb=xgb.pred, svm=svm.pred[,2], svmrbf=svm.rbf.pred[,2])
  ensemble.logis.pred<-predict(ensemble.logis.mod,ensemble.logis.test,family=binomial,type="response")
  ensemble.logis.pred
}
```

Next we look at the 6 dataframes which contains data for the actual matches played in each round of the 2017 NCAA March Madness tournament. They are already loaded in from the Rdata file. Each dataframe represents the matches played in the corresponding round in the tournament in a similar format as the train/test datasets. We predicted the probability of team 1 winning each match-up using the function that we created above. The accuracy of our model in predicting the results of the 2017 NCAA March Madness tournanment is determined. 

**As the most important variable in our model is the "PureSeed.diff" variable, our model is not very effective in predicting upsets**. We define upsets as a win for a lower seeded team. In 2017 NCAA March Madness, there were many upsets in the regional matches where a highly seeded team manage to emerge victorious against a lower seed, explaining the low accuracy in the first 4 rounds("First round" to "Elite Eight").

For the inter-regional matches in the national semifinals("Final Four") and national finals, PureSeed.diff is small since only the best teams make it to these round. Our model is very effective and manage to predict all the results correctly.

```{r}
##round 1
test<-round1[,c(-1,-2,-3,-26)]
prob1<-model.test(test)
round1.test<-cbind(round1[,c(2,3)],prob1,Actual = round1[,26])
round1.acc<-mean((round1.test$prob1>0.5) == round1.test$Actual)

##round 2
test<-round2[,c(-1,-2,-3,-26)]
prob2<-model.test(test)
round2.test<-cbind(round2[,c(2,3)],prob2,Actual = round2[,26])
round2.acc<-mean((round2.test$prob2>0.5) == round2.test$Actual)

##round 3
test<-round3[,c(-1,-2,-3,-26)]
prob3<-model.test(test)
round3.test<-cbind(round3[,c(2,3)],prob3,Actual = round3[,26])
round3.acc<-mean((round3.test$prob3>0.5) == round3.test$Actual)

##round 4
test<-round4[,c(-1,-2,-3,-26)]
prob4<-model.test(test)
round4.test<-cbind(round4[,c(2,3)],prob4,Actual = round4[,26])
round4.acc<-mean((round4.test$prob4>0.5) == round4.test$Actual)

##round 5
test<-round5[,c(-1,-2,-3,-26)]
prob5<-model.test(test)
round5.test<-cbind(round5[,c(2,3)],prob5,Actual = round5[,26])
round5.acc<-mean((round5.test$prob5>0.5) == round5.test$Actual)

##round 6
test<-round6[,c(-1,-2,-3,-26)]
prob6<-model.test(test)
round6.test<-cbind(round6[,c(2,3)],prob6,Actual = round6[,26])
round6.acc<-mean((round6.test$prob6>0.5) == round6.test$Actual)

table2<-data.frame(Round = c("First Round","Second Round","Sweet Sixteen","Elite Eight","Final Four"," NCAA Finals"), Accuracy = c(round1.acc, round2.acc, round3.acc, round4.acc, round5.acc, round6.acc))
table2

```

### Tournament run
We also generated a fictitious run of the tournament using our model. Starting with the given match-ups in round 1, we determine the match-ups in future rounds based on our model predictions. We determine the number of correct teams and also the number of correct matchups in each round compared to the actual tournament run. We found that our model-determined tournament run does not match very well with the actual tournament run as there were many upsets in the first few rounds. However,
we are still able to correctly predict North Carolina as the final winner.

```{r}
##round2
round2.ID<-c(round2$Team1,round2$Team2)
count.team<-0
count.match<-0
for (i in 1:16){
  count.team<-ifelse(tourney.run$round2.team1[i] %in% round2.ID,1,0)+count.team
  count.team<-ifelse(tourney.run$round2.team2[i] %in% round2.ID,1,0)+count.team
  count.match<-ifelse((tourney.run$round2.team1[i] %in% round2$Team1) & (tourney.run$round2.team2[i] %in% round2$Team2),1,0)+count.match
}
correct.team2<-count.team/32
correct.match2<-count.match/16

##round3
round3.ID<-c(round3$Team1,round3$Team2)
count.team<-0
count.match<-0
for (i in 1:8){
  count.team<-ifelse(tourney.run$round3.team1[i] %in% round3.ID,1,0)+count.team
  count.team<-ifelse(tourney.run$round3.team2[i] %in% round3.ID,1,0)+count.team
  count.match<-ifelse((tourney.run$round3.team1[i] %in% round3$Team1) & (tourney.run$round3.team2[i] %in% round3$Team2),1,0)+count.match
}
correct.team3<-count.team/16
correct.match3<-count.match/8

##round4
round4.ID<-c(round4$Team1,round4$Team2)
count.team<-0
count.match<-0
for (i in 1:4){
  count.team<-ifelse(tourney.run$round4.team1[i] %in% round4.ID,1,0)+count.team
  count.team<-ifelse(tourney.run$round4.team2[i] %in% round4.ID,1,0)+count.team
  count.match<-ifelse((tourney.run$round4.team1[i] %in% round4$Team1) & (tourney.run$round4.team2[i] %in% round4$Team2),1,0)+count.match
}
correct.team4<-count.team/8
correct.match4<-count.match/4

##round5
round5.ID<-c(round5$Team1,round5$Team2)
count.team<-0
count.match<-0
for (i in 1:2){
  count.team<-ifelse(tourney.run$round5.team1[i] %in% round5.ID,1,0)+count.team
  count.team<-ifelse(tourney.run$round5.team2[i] %in% round5.ID,1,0)+count.team
  count.match<-ifelse((tourney.run$round5.team1[i] %in% round5$Team1) & (tourney.run$round5.team2[i] %in% round5$Team2),1,0)+count.match
}
correct.team5<-count.team/4
correct.match5<-count.match/2

##round6
round6.ID<-c(round6$Team1,round6$Team2)
count.team<-0
count.match<-0
count.team<-ifelse(tourney.run$round6.team1[i] %in% round6.ID,1,0)+count.team
count.team<-ifelse(tourney.run$round6.team2[i] %in% round6.ID,1,0)+count.team
count.match<-ifelse((tourney.run$round6.team1[i] %in% round6$Team1) & (tourney.run$round6.team2[i] %in% round6$Team2),1,0)+count.match
correct.team6<-count.team/2
correct.match6<-count.match

##Winner
correct.team7<-tourney.run$winner[1]==1314
correct.match7<-tourney.run$winner[1]==1314

table3<-data.frame(Round = c("First Round","Second Round","Sweet Sixteen","Elite Eight","Final Four"," NCAA Finals","Winner"), Correct_Teams = c(NA, correct.team2, correct.team3, correct.team4, correct.team5, correct.team6, correct.team7))
table3

table4<-data.frame(Round = c("First Round","Second Round","Sweet Sixteen","Elite Eight","Final Four"," NCAA Finals","Winner"), Correct_Matchups = c(NA, correct.match2, correct.match3, correct.match4, correct.match5, correct.match6, correct.match7))
table4
```

## Conclusion

We built an ensemble model based on logistic regression, elastic net, random forest, Boosted Trees, SVM(Linear kernel) and SVM(RBF kernel) to predict NCAA tournament results with a test error of 27.9%. When our model is used to predict 2017 NCAA tournament results, we found that our model is not able to effectively predict upsets as the the seed difference between the teams is by far the most important variable in our model. However, our model appears to work well in the last 2 rounds when seed differences are not that big.

To improve our model, we could look to include more features in our model such as the markov features that takes into account the performance of the teams in the previous round of the tournament. Other basketball game statistics could be included if data becomes available to a build a model based more on team play instead of being largerly driven by seed differences between the teams.


***[THE END]***